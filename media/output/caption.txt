 In this paper, the authors present a new machine translation model called the Transformer. Unlike most existing models, which use recurrent or convolutional neural networks with an encoder-decoder structure, the Transformer relies entirely on self-attention mechanisms. This architecture allows for parallelization and faster training, as it reduces the sequential computation required for sequence modeling tasks.

The Transformer is composed of stacked self-attention and feed-forward layers for both the encoder and decoder. The self-attention sub-layers draw on inputs from different positions of the input and output sequences, allowing the model to focus on relevant dependencies regardless of their distance. The Transformer exhibits better quality and parallelizability, surpassing the existing state-of-the-art models on two machine translation tasks with significantly less training time. Specifically, on the English-to-German translation task, the Transformer achieved a BLEU score of 28.4, and on the English-to-French translation task, it reached a score of 41.0, outperforming previous single models at a fraction of the training cost.

The authors explored the importance of different components of the Transformer and found that a suitable number of attention heads, attention key and value dimensions, and learning rates are crucial for optimal performance. Furthermore, they employ three types of regularization during training: residual dropout, label smoothing, and applying the model’s learned positional encodings to the input instead of using learned positional embeddings had little effects on the model’s performance.

Overall, the Transformer is a novel and efficient machine translation model based solely on self-attention, improving both quality and training time compared to existing approaches.